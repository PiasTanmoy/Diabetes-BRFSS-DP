{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/home/tanmoysarkar/Trustworthiness/Diabetes - Paper/Gender Disparity in Diabetes/ML4H 23 (Submission files)/Code\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.isotonic import IsotonicRegression as IR\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, \\\n",
    "                            average_precision_score, accuracy_score, \\\n",
    "                            balanced_accuracy_score, roc_auc_score, \\\n",
    "                            matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, PrecisionRecallDisplay, roc_curve, auc\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings (not recommended for production)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress specific warnings by category (more targeted)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Example for UserWarning category\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "%cd /raid/home/tanmoysarkar/Trustworthiness/Diabetes - Paper/Gender Disparity in Diabetes/ML4H 23 (Submission files)/Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200136, 30) (210874, 30) (238130, 30) (220390, 30)\n",
      "(200136, 49) (210874, 49) (238130, 49) (220390, 49)\n",
      "(200136, 50) (210874, 50) (238130, 50) (220390, 50)\n"
     ]
    }
   ],
   "source": [
    "brfss_2021_mapped_cat = pd.read_csv('../../BRFSS Datasets - Cleaned/Mapped categories (Step 2)/BRFSS 2021.csv')\n",
    "\n",
    "brfss_2021_mapped_cat['Race'] = brfss_2021_mapped_cat['Race'].replace({1: 'White', 2: 'Black', 3: 'American Indian',\n",
    "                                                               4: 'Asian', 5: 'Native Hawaiian', 6: 'Other race',\n",
    "                                                               7: 'Multiracial', 8: 'Hispanic'})\n",
    "\n",
    "brfss_2019_mapped_cat = pd.read_csv('../../BRFSS Datasets - Cleaned/Mapped categories (Step 2)/BRFSS 2019.csv')\n",
    "\n",
    "brfss_2019_mapped_cat['Race'] = brfss_2019_mapped_cat['Race'].replace({1: 'White', 2: 'Black', 3: 'American Indian',\n",
    "                                                               4: 'Asian', 5: 'Native Hawaiian', 6: 'Other race',\n",
    "                                                               7: 'Multiracial', 8: 'Hispanic'})\n",
    "\n",
    "brfss_2017_mapped_cat = pd.read_csv('../../BRFSS Datasets - Cleaned/Mapped categories (Step 2)/BRFSS 2017.csv')\n",
    "\n",
    "brfss_2017_mapped_cat['Race'] = brfss_2017_mapped_cat['Race'].replace({1: 'White', 2: 'Black', 3: 'American Indian',\n",
    "                                                               4: 'Asian', 5: 'Native Hawaiian', 6: 'Other race',\n",
    "                                                               7: 'Multiracial', 8: 'Hispanic'})\n",
    "\n",
    "brfss_2015_mapped_cat = pd.read_csv('../../BRFSS Datasets - Cleaned/Mapped categories (Step 2)/BRFSS 2015.csv')\n",
    "\n",
    "brfss_2015_mapped_cat['Race'] = brfss_2015_mapped_cat['Race'].replace({1: 'White', 2: 'Black', 3: 'American Indian',\n",
    "                                                               4: 'Asian', 5: 'Native Hawaiian', 6: 'Other race',\n",
    "                                                               7: 'Multiracial', 8: 'Hispanic'})\n",
    "\n",
    "print(brfss_2021_mapped_cat.shape, brfss_2019_mapped_cat.shape, brfss_2017_mapped_cat.shape, brfss_2015_mapped_cat.shape)\n",
    "\n",
    "brfss_2021_one_hot = pd.read_csv('../../BRFSS Datasets - Cleaned/One hot encoded (Step 3)/BRFSS 2021.csv')\n",
    "brfss_2019_one_hot = pd.read_csv('../../BRFSS Datasets - Cleaned/One hot encoded (Step 3)/BRFSS 2019.csv')\n",
    "brfss_2017_one_hot = pd.read_csv('../../BRFSS Datasets - Cleaned/One hot encoded (Step 3)/BRFSS 2017.csv')\n",
    "brfss_2015_one_hot = pd.read_csv('../../BRFSS Datasets - Cleaned/One hot encoded (Step 3)/BRFSS 2015.csv')\n",
    "\n",
    "print(brfss_2021_one_hot.shape, brfss_2019_one_hot.shape, brfss_2017_one_hot.shape, brfss_2015_one_hot.shape)\n",
    "\n",
    "brfss_2021_one_hot['Race'] = brfss_2021_mapped_cat['Race'] # this is added to test performance over race\n",
    "brfss_2019_one_hot['Race'] = brfss_2019_mapped_cat['Race'] # this is added to test performance over race\n",
    "brfss_2017_one_hot['Race'] = brfss_2017_mapped_cat['Race'] # this is added to test performance over race\n",
    "brfss_2015_one_hot['Race'] = brfss_2015_mapped_cat['Race'] # this is added to test performance over race\n",
    "\n",
    "print(brfss_2021_one_hot.shape, brfss_2019_one_hot.shape, brfss_2017_one_hot.shape, brfss_2015_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics, Calibration, Threshold Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performace_metrix(y_true, y_pred, y_pred_proba, group = \"whole\"):\n",
    "  # Assuming you have true labels y_true and predicted labels y_pred\n",
    "  # For class 0 and class 1\n",
    "\n",
    "  #y_true, y_pred = y_test_2021, pred_test_2021_lr\n",
    "\n",
    "  # Calculate precision, recall, and f1-score for each class\n",
    "  precision_0 = precision_score(y_true, y_pred, pos_label=0)\n",
    "  precision_1 = precision_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "  recall_0 = recall_score(y_true, y_pred, pos_label=0)\n",
    "  recall_1 = recall_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "  f1_0 = f1_score(y_true, y_pred, pos_label=0)\n",
    "  f1_1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "  # Calculate Area Under the Precision-Recall Curve for each class\n",
    "  ap_0 = average_precision_score(y_true, y_pred_proba, pos_label=0)\n",
    "  ap_1 = average_precision_score(y_true, y_pred_proba, pos_label=1)\n",
    "\n",
    "  # Calculate overall accuracy\n",
    "  accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "  # Calculate balanced accuracy\n",
    "  balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "  # Calculate Area Under the ROC Curve\n",
    "  roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "  # Calculate Matthews Correlation Coefficient (MCC)\n",
    "  mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "  # Print the calculated metrics\n",
    "  print(\"Class 1 Recall:\", recall_1)\n",
    "  print(\"Class 1 Precision:\", precision_1)\n",
    "  print(\"Class 1 Area Under Precision-Recall Curve:\", ap_1)\n",
    "  print(\"Class 1 F1-Score:\", f1_1)\n",
    "\n",
    "\n",
    "  print(\"Class 0 Recall:\", recall_0)\n",
    "  print(\"Class 0 Precision:\", precision_0)\n",
    "  print(\"Class 0 Area Under Precision-Recall Curve:\", ap_0)\n",
    "  print(\"Class 0 F1-Score:\", f1_0)\n",
    "\n",
    "  print(\"Overall Accuracy:\", accuracy)\n",
    "  print(\"Balanced Accuracy:\", balanced_accuracy)\n",
    "  print(\"Area Under ROC Curve:\", roc_auc)\n",
    "  print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "\n",
    "\n",
    "  # Print the calculated metrics\n",
    "  print(recall_1)\n",
    "  print(precision_1)\n",
    "  print(ap_1)\n",
    "  print(f1_1)\n",
    "\n",
    "  print(recall_0)\n",
    "  print(precision_0)\n",
    "  print(ap_0)\n",
    "  print(f1_0)\n",
    "\n",
    "  print(accuracy)\n",
    "  print(balanced_accuracy)\n",
    "  print(roc_auc)\n",
    "  print(mcc)\n",
    "\n",
    "\n",
    "  performance = [group, recall_1, precision_1, ap_1, f1_1, recall_0, precision_0, ap_0, f1_0, accuracy, balanced_accuracy, roc_auc, mcc]\n",
    "  return performance\n",
    "\n",
    "def insert_performance_dict(performance_dict, performance):\n",
    "    c = 0\n",
    "    for k in performance_dict.keys():\n",
    "        performance_dict[k].append(performance[c])\n",
    "        c+=1\n",
    "    return performance_dict\n",
    "\n",
    "\n",
    "from sklearn.isotonic import IsotonicRegression as IR\n",
    "\n",
    "def isotonic(preds, labels, test_preds):\n",
    "    preds = np.array(preds)\n",
    "    labels = np.array(labels)\n",
    "    test_preds = np.array(test_preds)\n",
    "\n",
    "    ir = IR(out_of_bounds='clip')\n",
    "    ir.fit( preds, labels )\n",
    "\n",
    "    p_calibrated_v = ir.transform( preds )\n",
    "    p_calibrated_t = ir.transform( test_preds )   # or ir.fit( p_test ), that's the same thing\n",
    "\n",
    "    return p_calibrated_v, p_calibrated_t\n",
    "\n",
    "\n",
    "import heapq\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def Select_Threshold(df):\n",
    "    full_threshold_list = []\n",
    "    for threshold in np.arange(0,1.05,0.01):\n",
    "        #df.drop(columns = ['y_pred'])\n",
    "        df['y_pred'] = df['calibrated_prediction_prob'].apply(lambda x: 1 if x >= threshold else 0)\n",
    "        \n",
    "        y_pred = df[\"y_pred\"].values\n",
    "        y_true = df[\"y_true\"].values\n",
    "        \n",
    "        f1_C1 = f1_score(y_true, y_pred)\n",
    "        balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "        \n",
    "        full_threshold_list.append([threshold, f1_C1, balanced_accuracy])\n",
    "        \n",
    "    df_varying_threshold = pd.DataFrame(full_threshold_list, columns = ['threshold', 'f1_score', 'balanced_accuracy'])\n",
    "    \n",
    "    # select three highest F1 score and the the highest balanced accuracy\n",
    "    f1_scores = df_varying_threshold[\"f1_score\"].values\n",
    "    thresholds = df_varying_threshold[\"threshold\"].values\n",
    "    bal_acc_values = list(df_varying_threshold[\"balanced_accuracy\"].values)\n",
    "    \n",
    "    #print(heapq.nlargest(3, f1_scores))\n",
    "    list_index = heapq.nlargest(3, range(len(f1_scores)), key=f1_scores.__getitem__)\n",
    "    opt_threshold = thresholds[bal_acc_values.index(max(bal_acc_values[list_index[0]], bal_acc_values[list_index[1]], bal_acc_values[list_index[2]]))]\n",
    "    \n",
    "    \n",
    "    return opt_threshold, df_varying_threshold\n",
    "\n",
    "def pred_label_using_threshold(y_pred_p, th = 0.195):\n",
    "    y_pred_label = []\n",
    "    for y in y_pred_p:\n",
    "        if y <= th:\n",
    "            y_pred_label.append(0)\n",
    "        else:\n",
    "            y_pred_label.append(1)\n",
    "    return np.array(y_pred_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train, test, and validation set for entire experimental set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200136, 50) (120081, 50) (40027, 50) (40028, 50)\n",
      "(120081, 48) (40027, 48) (40028, 48)\n"
     ]
    }
   ],
   "source": [
    "random_state = 1\n",
    "\n",
    "df_train_2021, df_test_2021 = train_test_split(brfss_2021_one_hot, test_size=0.4, random_state=random_state)\n",
    "df_test_2021, df_validation_2021 = train_test_split(df_test_2021, test_size=0.5, random_state=random_state)\n",
    "print(brfss_2021_one_hot.shape, df_train_2021.shape, df_test_2021.shape, df_validation_2021.shape)\n",
    "\n",
    "X_train_2021 = df_train_2021.drop(['Diabetes', 'Race'],axis=1)\n",
    "y_train_2021 = df_train_2021['Diabetes']\n",
    "y_train_2021 = y_train_2021.to_numpy()\n",
    "\n",
    "X_test_2021 = df_test_2021.drop(['Diabetes', 'Race'],axis=1)\n",
    "y_test_2021 = df_test_2021['Diabetes']\n",
    "y_test_2021 = y_test_2021.to_numpy()\n",
    "\n",
    "X_validation_2021 = df_validation_2021.drop(['Diabetes', 'Race'],axis=1)\n",
    "y_validation_2021 = df_validation_2021['Diabetes']\n",
    "y_validation_2021 = y_validation_2021.to_numpy()\n",
    "\n",
    "print(X_train_2021.shape, X_test_2021.shape, X_validation_2021.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whole group threshold calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.24\n",
      "1 0.24\n",
      "2 0.24\n",
      "3 0.22\n",
      "4 0.24\n",
      "0.236\n"
     ]
    }
   ],
   "source": [
    "sum_ = 0\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    random_state = i\n",
    "\n",
    "    df_train_2021, df_test_2021 = train_test_split(brfss_2021_one_hot, test_size=0.4, random_state=random_state)\n",
    "    df_test_2021, df_validation_2021 = train_test_split(df_test_2021, test_size=0.5, random_state=random_state)\n",
    "\n",
    "    X_train_2021 = df_train_2021.drop(['Diabetes', 'Race'],axis=1)\n",
    "    y_train_2021 = df_train_2021['Diabetes']\n",
    "    y_train_2021 = y_train_2021.to_numpy()\n",
    "\n",
    "    X_test_2021 = df_test_2021.drop(['Diabetes', 'Race'],axis=1)\n",
    "    y_test_2021 = df_test_2021['Diabetes']\n",
    "    y_test_2021 = y_test_2021.to_numpy()\n",
    "\n",
    "    X_validation_2021 = df_validation_2021.drop(['Diabetes', 'Race'],axis=1)\n",
    "    y_validation_2021 = df_validation_2021['Diabetes']\n",
    "    y_validation_2021 = y_validation_2021.to_numpy()\n",
    "\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train_2021, y_train_2021)\n",
    "\n",
    "    pred_prob_test_lr = lr.predict_proba(X_test_2021)[:, 1]\n",
    "    pred_prob_validation_lr = lr.predict_proba(X_validation_2021)[:, 1]\n",
    "\n",
    "    p_calibrated_val, p_calibrated_test = isotonic(pred_prob_validation_lr, y_validation_2021, pred_prob_test_lr)\n",
    "\n",
    "    X_validation_2021_preds = X_validation_2021.copy()\n",
    "    X_validation_2021_preds['y_true'] = y_validation_2021\n",
    "    X_validation_2021_preds['calibrated_prediction_prob'] = p_calibrated_val\n",
    "\n",
    "    th, th_df = Select_Threshold(X_validation_2021_preds)\n",
    "    #th_df.to_csv(\"Threshold-selection/threshold_moving_trial_\"+str(random_state)+\".csv\", index = False)\n",
    "    print(i, th)\n",
    "    sum_ += th\n",
    "\n",
    "\n",
    "print(sum_ / (i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgroup threshold calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================  0 ============================ \n",
      "3 0.17\n",
      "4 0.21\n",
      "5 0.21\n",
      "============================  1 ============================ \n",
      "3 0.18\n",
      "4 0.21\n",
      "5 0.24\n",
      "============================  2 ============================ \n",
      "3 0.14\n",
      "4 0.24\n",
      "5 0.18\n",
      "============================  3 ============================ \n",
      "3 0.12\n",
      "4 0.18\n",
      "5 0.18\n",
      "============================  4 ============================ \n",
      "3 0.13\n",
      "4 0.2\n",
      "5 0.2\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "th_dict = defaultdict(list)\n",
    "\n",
    "for state in range(5):\n",
    "    print(\"============================ \", state , \"============================ \")\n",
    "    random_state = state\n",
    "\n",
    "    df_train_2021, df_test_2021 = train_test_split(brfss_2021_one_hot, test_size=0.4, random_state=random_state)\n",
    "    df_test_2021, df_validation_2021 = train_test_split(df_test_2021, test_size=0.5, random_state=random_state)\n",
    "\n",
    "    X_train_2021 = df_train_2021.drop(['Diabetes', 'Race'],axis=1)\n",
    "    y_train_2021 = df_train_2021['Diabetes']\n",
    "    y_train_2021 = y_train_2021.to_numpy()\n",
    "\n",
    "    X_test_2021 = df_test_2021.drop(['Diabetes', 'Race'],axis=1)\n",
    "    y_test_2021 = df_test_2021['Diabetes']\n",
    "    y_test_2021 = y_test_2021.to_numpy()\n",
    "\n",
    "    X_validation_2021 = df_validation_2021.drop(['Diabetes', 'Race'],axis=1)\n",
    "    y_validation_2021 = df_validation_2021['Diabetes']\n",
    "    y_validation_2021 = y_validation_2021.to_numpy()\n",
    "\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    lr.fit(X_train_2021, y_train_2021)\n",
    "\n",
    "    pred_prob_test_lr = lr.predict_proba(X_test_2021)[:, 1]\n",
    "    pred_prob_validation_lr = lr.predict_proba(X_validation_2021)[:, 1]\n",
    "\n",
    "    p_calibrated_val, p_calibrated_test = isotonic(pred_prob_validation_lr, y_validation_2021, pred_prob_test_lr)\n",
    "\n",
    "    X_validation_2021_preds = X_validation_2021.copy()\n",
    "    X_validation_2021_preds['y_true'] = y_validation_2021\n",
    "    X_validation_2021_preds['calibrated_prediction_prob'] = p_calibrated_val\n",
    "\n",
    "\n",
    "    age_group = [3, 4, 5]\n",
    "\n",
    "    for i in age_group:\n",
    "        \n",
    "        X_validation_2021_age_group = X_validation_2021_preds[X_validation_2021_preds.Age == i]\n",
    "        th, th_df = Select_Threshold(X_validation_2021_age_group)\n",
    "        #th_df.to_csv(\"Threshold-selection/threshold_moving_trial_\"+str(random_state)+\".csv\", index = False)\n",
    "        print(i, th)\n",
    "        th_dict[i].append(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.148\n",
      "4 0.20799999999999996\n",
      "5 0.20199999999999996\n"
     ]
    }
   ],
   "source": [
    "for k in th_dict.keys():\n",
    "    print(k, sum(th_dict[k]) / len(th_dict[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================  3 ============================ \n",
      "Class 1 Recall: 0.45614035087719296\n",
      "Class 1 Precision: 0.15757575757575756\n",
      "Class 1 Area Under Precision-Recall Curve: 0.15722124531485454\n",
      "Class 1 F1-Score: 0.2342342342342342\n",
      "Class 0 Recall: 0.9352283317800559\n",
      "Class 0 Precision: 0.9847890088321885\n",
      "Class 0 Area Under Precision-Recall Curve: 0.9378767316237041\n",
      "Class 0 F1-Score: 0.9593690248565966\n",
      "Overall Accuracy: 0.9228325011348162\n",
      "Balanced Accuracy: 0.6956843413286244\n",
      "Area Under ROC Curve: 0.8441776622357384\n",
      "Matthews Correlation Coefficient: 0.2360447226393439\n",
      "0.45614035087719296\n",
      "0.15757575757575756\n",
      "0.15722124531485454\n",
      "0.2342342342342342\n",
      "0.9352283317800559\n",
      "0.9847890088321885\n",
      "0.9378767316237041\n",
      "0.9593690248565966\n",
      "0.9228325011348162\n",
      "0.6956843413286244\n",
      "0.8441776622357384\n",
      "0.2360447226393439\n",
      "============================  4 ============================ \n",
      "Class 1 Recall: 0.4251968503937008\n",
      "Class 1 Precision: 0.2857142857142857\n",
      "Class 1 Area Under Precision-Recall Curve: 0.2644674089568087\n",
      "Class 1 F1-Score: 0.34177215189873417\n",
      "Class 0 Recall: 0.9500554938956715\n",
      "Class 0 Precision: 0.9723589549413101\n",
      "Class 0 Area Under Precision-Recall Curve: 0.892672244122633\n",
      "Class 0 F1-Score: 0.9610778443113771\n",
      "Overall Accuracy: 0.9265017667844523\n",
      "Balanced Accuracy: 0.6876261721446861\n",
      "Area Under ROC Curve: 0.8703350899117631\n",
      "Matthews Correlation Coefficient: 0.31119541859475963\n",
      "0.4251968503937008\n",
      "0.2857142857142857\n",
      "0.2644674089568087\n",
      "0.34177215189873417\n",
      "0.9500554938956715\n",
      "0.9723589549413101\n",
      "0.892672244122633\n",
      "0.9610778443113771\n",
      "0.9265017667844523\n",
      "0.6876261721446861\n",
      "0.8703350899117631\n",
      "0.31119541859475963\n",
      "============================  5 ============================ \n",
      "Class 1 Recall: 0.44776119402985076\n",
      "Class 1 Precision: 0.2694610778443114\n",
      "Class 1 Area Under Precision-Recall Curve: 0.28479657131497266\n",
      "Class 1 F1-Score: 0.33644859813084116\n",
      "Class 0 Recall: 0.9170914033299354\n",
      "Class 0 Precision: 0.9604982206405694\n",
      "Class 0 Area Under Precision-Recall Curve: 0.8609797541628502\n",
      "Class 0 F1-Score: 0.9382930644880931\n",
      "Overall Accuracy: 0.8870865139949109\n",
      "Balanced Accuracy: 0.6824262986798931\n",
      "Area Under ROC Curve: 0.840903366281065\n",
      "Matthews Correlation Coefficient: 0.2896571202978499\n",
      "0.44776119402985076\n",
      "0.2694610778443114\n",
      "0.28479657131497266\n",
      "0.33644859813084116\n",
      "0.9170914033299354\n",
      "0.9604982206405694\n",
      "0.8609797541628502\n",
      "0.9382930644880931\n",
      "0.8870865139949109\n",
      "0.6824262986798931\n",
      "0.840903366281065\n",
      "0.2896571202978499\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"Performance-LR/\"\n",
    "state = \"subgrup_threshold\"\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "df_train_2021, df_test_2021 = train_test_split(brfss_2021_one_hot, test_size=0.4, random_state=random_state)\n",
    "df_test_2021, df_validation_2021 = train_test_split(df_test_2021, test_size=0.5, random_state=random_state)\n",
    "\n",
    "X_train_2021 = df_train_2021.drop(['Diabetes', 'Race'],axis=1)\n",
    "y_train_2021 = df_train_2021['Diabetes']\n",
    "y_train_2021 = y_train_2021.to_numpy()\n",
    "\n",
    "X_test_2021 = df_test_2021.drop(['Diabetes', 'Race'],axis=1)\n",
    "y_test_2021 = df_test_2021['Diabetes']\n",
    "y_test_2021 = y_test_2021.to_numpy()\n",
    "\n",
    "X_validation_2021 = df_validation_2021.drop(['Diabetes', 'Race'],axis=1)\n",
    "y_validation_2021 = df_validation_2021['Diabetes']\n",
    "y_validation_2021 = y_validation_2021.to_numpy()\n",
    "\n",
    "\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(X_train_2021, y_train_2021)\n",
    "\n",
    "pred_prob_test_lr = lr.predict_proba(X_test_2021)[:, 1]\n",
    "pred_prob_validation_lr = lr.predict_proba(X_validation_2021)[:, 1]\n",
    "\n",
    "# calibration\n",
    "ir = IR(out_of_bounds='clip')\n",
    "ir.fit(pred_prob_validation_lr, y_validation_2021 )\n",
    "pred_prob_validation_lr_calibrated = ir.transform( pred_prob_validation_lr )\n",
    "pred_prob_test_lr_calibrated = ir.transform( pred_prob_test_lr )\n",
    "\n",
    "\n",
    "X_test_2021_preds = X_test_2021.copy()\n",
    "X_test_2021_preds['y_true'] = y_test_2021\n",
    "X_test_2021_preds['calibrated_prediction_prob'] = pred_prob_test_lr_calibrated\n",
    "\n",
    "performance_dict = {\n",
    "        \"Group \": [],\n",
    "        \"Class 1 Recall\": [],\n",
    "        \"Class 1 Precision\": [],\n",
    "        \"Class 1 AU_PRC\": [],\n",
    "        \"Class 1 F1-Score\": [],\n",
    "        \"Class 0 Recall\": [],\n",
    "        \"Class 0 Precision\": [],\n",
    "        \"Class 0 AU_PRC\": [],\n",
    "        \"Class 0 F1-Score\": [],\n",
    "        \"Overall Accuracy\": [],\n",
    "        \"Balanced Accuracy\": [],\n",
    "        \"AU_ROC\": [],\n",
    "        \"MCC\": []\n",
    "    }\n",
    "\n",
    "age_group_th = {\n",
    "    3: 0.148,\n",
    "    4: 0.21,\n",
    "    5: 0.20\n",
    "}\n",
    "\n",
    "for i in age_group_th.keys():\n",
    "    test_group = X_test_2021_preds[X_test_2021_preds.Age == i]\n",
    "    print(\"============================ \", i , \"============================ \")\n",
    "\n",
    "    X_test_group = X_test_2021_preds.drop(['y_true', 'calibrated_prediction_prob'],axis=1)\n",
    "\n",
    "    y_test_group = test_group['y_true']\n",
    "    y_test_group = y_test_group.to_numpy()\n",
    "\n",
    "    y_test_pred_calibrated_group = test_group['calibrated_prediction_prob']\n",
    "    y_test_pred_calibrated_group = y_test_pred_calibrated_group.to_numpy()\n",
    "\n",
    "    # thresholding\n",
    "    pred_test_group = pred_label_using_threshold(y_test_pred_calibrated_group, age_group_th[i])\n",
    "\n",
    "    performace = performace_metrix(y_test_group, pred_test_group, y_test_pred_calibrated_group, i)\n",
    "    performance_dict = insert_performance_dict(performance_dict, performace)\n",
    "\n",
    "all_performances = pd.DataFrame(performance_dict)\n",
    "all_performances.to_csv(save_dir + state +  \".csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
